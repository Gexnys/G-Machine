# "G-Machine: Extreme ViT Confusion Matrix, Vision Transformer Machine Learning"

# 1. Data Preparation
‚Ä¢ Downloads the MNIST dataset, which contains handwritten digit images.

‚Ä¢ Normalizes the pixel values to have zero mean and unit variance.

‚Ä¢ Splits the data into 80% training and 20% testing sets.

‚Ä¢ Converts the data into PyTorch tensors and batches it for efficient processing.

# 2. Model Setup
‚Ä¢ Builds a Vision Transformer (ViT) model that divides each image into small patches and treats them as a sequence.

‚Ä¢ Uses transformer encoder blocks to learn global relationships between these patches via self-attention mechanisms.

‚Ä¢ Ends with fully connected layers for classifying the digit classes.

# 3. Training the Model
‚Ä¢ Moves the model to GPU (if available) or CPU.

‚Ä¢ Uses the AdamW optimizer and CrossEntropyLoss to train the model.

‚Ä¢ Applies a learning rate scheduler (CosineAnnealingWarmRestarts) to adjust the learning rate over time.

‚Ä¢ For each epoch (one full pass over the training data), the model:

‚Ä¢ Makes predictions on batches of images.

‚Ä¢ Calculates the loss (error) between predictions and true labels.

‚Ä¢ Updates model weights to minimize the loss.

‚Ä¢ Prints the loss and accuracy at the end of each epoch.

# 4. Evaluating the Model
‚Ä¢ After training, the model predicts labels for the unseen test dataset.

‚Ä¢ Calculates performance metrics like accuracy and weighted F1 score.

‚Ä¢ Generates a confusion matrix plot showing which digits the model predicts correctly or confuses.

# 5. What changed in the G-Machine update 0.2

‚Ä¢ Data Augmentation was added (RandomRotation, RandomAffine, RandomErasing) ‚Üí stronger generalization.

‚Ä¢ A lighter & deeper ViT (embedding size = 192, depth = 12, heads = 6) was used ‚Üí balance between speed and accuracy.

‚Ä¢ Stochastic Depth (DropPath) ‚Üí reduced Transformer overfitting.

‚Ä¢ Label Smoothing ‚Üí CrossEntropyLoss(label_smoothing=0.1) was applied to prevent overfitting.

‚Ä¢ Gradient Clipping was added ‚Üí to prevent exploding gradients.

‚Ä¢ Early Stopping ‚Üí training stops if no improvement is observed for 5 epochs.

‚Ä¢ Test-Time Augmentation (TTA) ‚Üí multiple predictions under small rotations were averaged during inference.

‚Ä¢ TensorBoard logger was added ‚Üí to visualize loss and accuracy.

# G-Machine ‚Äì Version 0.3
# The Next Generation of Artificial Intelligence

üöÄ What's New

- Vision Transformer AI: Advanced model for analyzing images through deep learning

- Smart Search: Real-time information querying with Google integration

- Code Generation: Python, HTML and CSS template support

- Modern Interface: Fast, clean and user-friendly design

‚öôÔ∏è Technical Specifications

- Local server support (localhost:5000)

- Python 3.8+ compatibility

- Real-time chat infrastructure

- Automatic information synthesis system

# ¬© 2025 Gexnys

# This code was written by Gexnys.
‚Ä¢ Email address: developergokhan@proton.me


